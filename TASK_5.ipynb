{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoZGB0JPl3vy"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Gurupatil0003/FakeFace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Flatten,Dense,Conv2D,Conv2DTranspose,Input,Reshape,BatchNormalization,LeakyReLU"
      ],
      "metadata": {
        "id": "OZ8-Jy3YrES-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_ds=tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/content/FakeFace\",\n",
        "    image_size=(64,64),\n",
        "    batch_size=64,\n",
        "    label_mode=None\n",
        ").map(lambda x:(tf.cast(x,tf.float32)/127.5)-1).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "rYqGr1JVrGdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(latent=100):\n",
        "  return tf.keras.models.Sequential([\n",
        "      Input((latent,)),\n",
        "      Dense(4*4*512,use_bias=False),\n",
        "      Reshape((4,4,512)),\n",
        "      Conv2DTranspose(256,4,2,padding=\"same\",use_bias=False),\n",
        "      BatchNormalization(),\n",
        "      LeakyReLU(),\n",
        "      Conv2DTranspose(128,4,2,padding=\"same\",use_bias=False),\n",
        "      BatchNormalization(),\n",
        "      LeakyReLU(),\n",
        "      Conv2DTranspose(64,4,2,padding=\"same\",use_bias=False),\n",
        "      BatchNormalization(),\n",
        "      LeakyReLU(),\n",
        "      Conv2DTranspose(3,4,2,padding=\"same\",activation=\"tanh\")\n",
        "\n",
        "\n",
        "\n",
        "  ])\n",
        "\n",
        "def build_discriminator(img=64):\n",
        "   return tf.keras.models.Sequential([\n",
        "        Input((img,img,3)),\n",
        "        Conv2D(64,4,2,padding=\"same\"),LeakyReLU(0.2),\n",
        "        Conv2D(128,4,2,padding=\"same\"),LeakyReLU(0.2),\n",
        "        Conv2D(256,4,2,padding=\"same\"),LeakyReLU(0.2),\n",
        "        Conv2D(512,4,2,padding=\"same\"),LeakyReLU(0.2),\n",
        "        Flatten(),\n",
        "        Dense(1)\n",
        "   ])\n",
        "\n",
        "G,D=build_generator(),build_discriminator()\n"
      ],
      "metadata": {
        "id": "509WA3MmrIQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "g_opt=tf.keras.optimizers.Adam(2e-4,0.5)\n",
        "d_opt=tf.keras.optimizers.Adam(2e-4,0.5)\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def data_test(real):\n",
        "  noise=tf.random.normal([64,100])\n",
        "  with tf.GradientTape() as g_tape:\n",
        "    fake=G(noise,training=True)\n",
        "    fake_logits=D(fake,training=True)\n",
        "    g_loss=loss_fn(tf.ones_like(fake_logits),fake_logits)\n",
        "\n",
        "  with tf.GradientTape() as d_tape:\n",
        "    fake=G(noise,training=True)\n",
        "    real_logits=D(real,training=True)\n",
        "    fake_logits=D(fake,training=True)\n",
        "\n",
        "    real_loss=loss_fn(tf.ones_like(real_logits),real_logits)\n",
        "    fake_loss=loss_fn(tf.zeros_like(fake_logits),fake_logits)\n",
        "    d_loss = real_loss + fake_loss\n",
        "\n",
        "\n",
        "  g_opt.apply_gradients(zip(g_tape.gradient(g_loss,G.trainable_variables),G.trainable_variables))\n",
        "  d_opt.apply_gradients(zip(d_tape.gradient(d_loss,D.trainable_variables),D.trainable_variables))\n",
        "  return g_loss, d_loss"
      ],
      "metadata": {
        "id": "CKzeA-TrrKZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=70\n",
        "import matplotlib.pyplot as plt\n",
        "seed=tf.random.normal([64,100])\n",
        "for epoch in range(EPOCHS):\n",
        "  for real in img_ds:\n",
        "    g_loss, d_loss=data_test(real)\n",
        "    print(f\"{epoch+1}/{EPOCHS} G:{g_loss} , D:{d_loss}\")\n",
        "    if(epoch+1)%2==0:\n",
        "      img=(G(seed,training=False))\n",
        "      plt.figure(figsize=(4,4))\n",
        "      for i in range(16):\n",
        "        plt.subplot(4,4,i+1)\n",
        "        plt.imshow((img[i]+1)/2)\n",
        "        plt.axis(\"off\")\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "a0G0LYjHre6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}