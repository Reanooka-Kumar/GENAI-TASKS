{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cMK4iuJsv0Uy"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch datasets soundfile torchaudio pillow scipy gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoConfig\n",
        "\n",
        "# List of 10 core Transformer architectures\n",
        "core_model_names = [\n",
        "    \"bert-base-uncased\", \"gpt2\", \"t5-small\", \"roberta-base\",\n",
        "    \"distilbert-base-uncased\", \"google/vit-base-patch16-224\",\n",
        "    \"facebook/bart-base\", \"google/electra-small-discriminator\",\n",
        "    \"albert-base-v2\", \"microsoft/deberta-v3-small\"\n",
        "]\n",
        "\n",
        "print(\"--- Verifying Core Architectures ---\")\n",
        "for name in core_model_names:\n",
        "    try:\n",
        "        # Loading config to save memory, but verifies the model is accessible\n",
        "        config = AutoConfig.from_pretrained(name)\n",
        "        print(f\"‚úÖ Architecture Verified: {name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {name}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p23Og0UFv6UC",
        "outputId": "3b302024-5764-462e-8b90-d27ae2866d92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verifying Core Architectures ---\n",
            "‚úÖ Architecture Verified: bert-base-uncased\n",
            "‚úÖ Architecture Verified: gpt2\n",
            "‚úÖ Architecture Verified: t5-small\n",
            "‚úÖ Architecture Verified: roberta-base\n",
            "‚úÖ Architecture Verified: distilbert-base-uncased\n",
            "‚úÖ Architecture Verified: google/vit-base-patch16-224\n",
            "‚úÖ Architecture Verified: facebook/bart-base\n",
            "‚úÖ Architecture Verified: google/electra-small-discriminator\n",
            "‚úÖ Architecture Verified: albert-base-v2\n",
            "‚úÖ Architecture Verified: microsoft/deberta-v3-small\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "\n",
        "print(\"Loading Speech Model (Text-to-Audio)...\")\n",
        "tts_pipe = pipeline(\"text-to-speech\", model=\"facebook/mms-tts-eng\", device=device)\n",
        "\n",
        "print(\"Loading Music Model (Music Generation)...\")\n",
        "music_pipe = pipeline(\"text-to-audio\", model=\"facebook/musicgen-small\", device=device)\n",
        "\n",
        "print(\"Loading VQA Model (Questioning Images)...\")\n",
        "vqa_pipe = pipeline(\"visual-question-answering\", model=\"dandelin/vilt-b32-mlm\", device=device)\n",
        "\n",
        "print(\"Loading Captioning Model (Image-to-Text)...\")\n",
        "caption_pipe = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\", device=device)\n",
        "\n",
        "print(\"Loading Audio Classifier...\")\n",
        "audio_classifier = pipeline(\"audio-classification\", model=\"MIT/ast-finetuned-audioset-10-10-0.4593\", device=device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEX3OY6Av8Iu",
        "outputId": "585e630c-c315-47ab-d972-2572d316c8ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: GPU\n",
            "Loading Speech Model (Text-to-Audio)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Music Model (Music Generation)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading VQA Model (Questioning Images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.1.bias', 'classifier.1.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Captioning Model (Image-to-Text)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Audio Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import scipy.io.wavfile\n",
        "\n",
        "# --- Logic Functions ---\n",
        "\n",
        "def process_speech(text):\n",
        "    out = tts_pipe(text)\n",
        "    return (out[\"sampling_rate\"], out[\"audio\"].T)\n",
        "\n",
        "def process_music(prompt):\n",
        "    # Generates a short clip for testing\n",
        "    out = music_pipe(prompt, forward_params={\"max_new_tokens\": 256})\n",
        "    return (out[\"sampling_rate\"], out[\"audio\"].T)\n",
        "\n",
        "def process_vision(img, question):\n",
        "    # Task 1: Generate a description\n",
        "    caption = caption_pipe(img)[0]['generated_text']\n",
        "    # Task 2: Answer specific question\n",
        "    answer = \"N/A\"\n",
        "    if question:\n",
        "        res = vqa_pipe(img, question=question, top_k=1)\n",
        "        answer = res[0]['answer']\n",
        "    return caption, answer\n",
        "\n",
        "def process_classify(audio_path):\n",
        "    if audio_path is None: return \"Please record/upload audio\"\n",
        "    label = audio_classifier(audio_path)\n",
        "    return {l['label']: l['score'] for l in label}\n",
        "\n",
        "# --- Build the UI ---\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ü§ñ 15-Model AI Playground\")\n",
        "\n",
        "    with gr.Tab(\"üéôÔ∏è Speech (TTS)\"):\n",
        "        t_in = gr.Textbox(label=\"Enter Text\", value=\"Transformers are powerful models.\")\n",
        "        s_out = gr.Audio(label=\"AI Voice\")\n",
        "        gr.Button(\"Generate\").click(process_speech, t_in, s_out)\n",
        "\n",
        "    with gr.Tab(\"üéµ Music Gen\"):\n",
        "        m_in = gr.Textbox(label=\"Music Style\", placeholder=\"Lo-fi hip hop with piano\")\n",
        "        m_out = gr.Audio(label=\"AI Music\")\n",
        "        gr.Button(\"Compose\").click(process_music, m_in, m_out)\n",
        "\n",
        "    with gr.Tab(\"üñºÔ∏è Vision (VQA/Caption)\"):\n",
        "        with gr.Row():\n",
        "            i_in = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
        "            with gr.Column():\n",
        "                q_in = gr.Textbox(label=\"Ask about the image\")\n",
        "                c_out = gr.Textbox(label=\"AI Caption\")\n",
        "                a_out = gr.Textbox(label=\"Answer\")\n",
        "        gr.Button(\"Analyze\").click(process_vision, [i_in, q_in], [c_out, a_out])\n",
        "\n",
        "    with gr.Tab(\"üéß Audio Classifier\"):\n",
        "        aud_in = gr.Audio(type=\"filepath\", label=\"Record Audio\")\n",
        "        lab_out = gr.Label(label=\"Detected Sounds\")\n",
        "        gr.Button(\"Identify\").click(process_classify, aud_in, lab_out)\n",
        "\n",
        "# Launch with a shareable link\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "qFUzkalFwAUs",
        "outputId": "26542774-c0ab-4582-a8dc-4da6196b2370"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3557306429.py:33: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://454e85133a0649002c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://454e85133a0649002c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://454e85133a0649002c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}